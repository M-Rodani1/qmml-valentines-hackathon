{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade lightgbm for CUDA support (4.0+ has it built-in)\n",
    "!pip install -q --upgrade lightgbm\n",
    "!pip install -q catboost optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "from scipy.stats import rankdata\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU is available\n",
    "# If this shows 'No GPU' go to Runtime -> Change runtime type -> GPU\n",
    "import subprocess\n",
    "result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total',\n",
    "                         '--format=csv,noheader'], capture_output=True, text=True)\n",
    "if result.returncode == 0:\n",
    "    print(f'GPU detected: {result.stdout.strip()}')\n",
    "    DEVICE = 'gpu'\n",
    "else:\n",
    "    print('No GPU found â€” go to Runtime -> Change runtime type -> T4 GPU')\n",
    "    DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Option A: Upload files manually ---\n",
    "# from google.colab import files\n",
    "# files.upload()  # upload train.csv and test.csv\n",
    "\n",
    "# --- Option B: Kaggle API (recommended) ---\n",
    "# 1. Go to kaggle.com -> Settings -> API -> Create New Token -> downloads kaggle.json\n",
    "# 2. Run this cell\n",
    "from google.colab import files\n",
    "files.upload()  # upload your kaggle.json here\n",
    "\n",
    "!mkdir -p ~/.kaggle\n",
    "!mv kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle competitions download -c valentine-hackathon-2026\n",
    "!unzip -q valentine-hackathon-2026.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test  = pd.read_csv('test.csv')\n",
    "print(f'Train: {train.shape} | Test: {test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    df['Survey_Date']      = pd.to_datetime(df['Survey_Date'], errors='coerce')\n",
    "    df['Survey_Month']     = df['Survey_Date'].dt.month\n",
    "    df['Survey_Hour']      = df['Survey_Date'].dt.hour\n",
    "    df['Survey_DayOfWeek'] = df['Survey_Date'].dt.dayofweek\n",
    "\n",
    "    df['BMI']            = df['Weight_kg'] / ((df['Height_cm'] / 100) ** 2)\n",
    "    df['Log_Income']     = np.log1p(df['Income'])\n",
    "    df['Income_per_Age'] = df['Income'] / (df['Age'] + 1)\n",
    "\n",
    "    df['Appear_x_Social']   = df['Appearance_Score']   * df['Social_Skills_Score']\n",
    "    df['Dating_x_Extra']    = df['Dating_App_User']    * df['Extraversion_Score']\n",
    "    df['Extra_x_Social']    = df['Extraversion_Score'] * df['Social_Skills_Score']\n",
    "    df['Dating_x_Prev_Rel'] = df['Dating_App_User']    * df['Previous_Relationships']\n",
    "\n",
    "    df['Social_Composite'] = (\n",
    "        df['Social_Skills_Score'].fillna(0) + df['Extraversion_Score'].fillna(0)\n",
    "    ) / 2\n",
    "\n",
    "    for col in ['Pets', 'Favorite_Color', 'Social_Media_Presence',\n",
    "                'Zodiac_Sign', 'Social_Skills_Score', 'Appearance_Score']:\n",
    "        df[f'{col}_missing'] = df[col].isnull().astype(int)\n",
    "\n",
    "    df['n_missing'] = df.isnull().sum(axis=1)\n",
    "    return df\n",
    "\n",
    "train = engineer_features(train)\n",
    "test  = engineer_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_encode_cols = ['Zodiac_Sign', 'Pets', 'Favorite_Color', 'Social_Media_Presence',\n",
    "                      'Gender', 'Education', 'Job_Type', 'Location_Type']\n",
    "\n",
    "skf_te      = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "global_mean = train['Has_Valentine'].mean()\n",
    "\n",
    "for col in target_encode_cols:\n",
    "    train[f'{col}_te'] = np.nan\n",
    "    for tr_idx, val_idx in skf_te.split(train, train['Has_Valentine']):\n",
    "        col_means = train.iloc[tr_idx].groupby(col)['Has_Valentine'].mean()\n",
    "        train.loc[train.index[val_idx], f'{col}_te'] = (\n",
    "            train.iloc[val_idx][col].map(col_means).fillna(global_mean)\n",
    "        )\n",
    "    col_means_full    = train.groupby(col)['Has_Valentine'].mean()\n",
    "    test[f'{col}_te'] = test[col].map(col_means_full).fillna(global_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['Gender', 'Education', 'Job_Type', 'Social_Media_Presence',\n",
    "            'Location_Type', 'Zodiac_Sign', 'Pets', 'Favorite_Color']\n",
    "\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    combined = pd.concat([train[col], test[col]], axis=0).astype(str)\n",
    "    le.fit(combined)\n",
    "    train[col] = le.transform(train[col].astype(str))\n",
    "    test[col]  = le.transform(test[col].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols    = ['Id', 'Has_Valentine', 'Survey_Date']\n",
    "feature_cols = [c for c in train.columns if c not in drop_cols]\n",
    "\n",
    "X        = train[feature_cols]\n",
    "y        = train['Has_Valentine']\n",
    "X_test   = test[feature_cols]\n",
    "test_ids = test['Id']\n",
    "cv       = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "tune_train_idx, tune_val_idx = next(cv.split(X, y))\n",
    "X_tr, X_val = X.iloc[tune_train_idx], X.iloc[tune_val_idx]\n",
    "y_tr, y_val = y.iloc[tune_train_idx], y.iloc[tune_val_idx]\n",
    "\n",
    "print(f'Features: {len(feature_cols)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_objective(trial):\n",
    "    params = {\n",
    "        'num_leaves':        trial.suggest_int('num_leaves', 31, 256),\n",
    "        'learning_rate':     trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 20, 200),\n",
    "        'feature_fraction':  trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
    "        'bagging_fraction':  trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
    "        'bagging_freq':      trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'reg_lambda':        trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),\n",
    "        'reg_alpha':         trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),\n",
    "        'n_estimators': 1000, 'random_state': 42, 'verbose': -1,\n",
    "        'device': DEVICE,\n",
    "    }\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)],\n",
    "              callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "    return roc_auc_score(y_val, model.predict_proba(X_val)[:, 1])\n",
    "\n",
    "lgbm_study = optuna.create_study(direction='maximize')\n",
    "lgbm_study.optimize(lgbm_objective, n_trials=50, show_progress_bar=True)\n",
    "print(f'LGBM Best AUC: {lgbm_study.best_value:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_objective(trial):\n",
    "    params = {\n",
    "        'max_depth':        trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate':    trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'subsample':        trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n",
    "        'reg_alpha':        trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),\n",
    "        'reg_lambda':       trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),\n",
    "        'gamma':            trial.suggest_float('gamma', 0.0, 1.0),\n",
    "        'n_estimators': 1000, 'random_state': 42, 'verbosity': 0,\n",
    "        'tree_method': 'hist', 'device': 'cuda' if DEVICE == 'gpu' else 'cpu',\n",
    "        'early_stopping_rounds': 50, 'eval_metric': 'auc',\n",
    "    }\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    return roc_auc_score(y_val, model.predict_proba(X_val)[:, 1])\n",
    "\n",
    "xgb_study = optuna.create_study(direction='maximize')\n",
    "xgb_study.optimize(xgb_objective, n_trials=50, show_progress_bar=True)\n",
    "print(f'XGB  Best AUC: {xgb_study.best_value:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_objective(trial):\n",
    "    params = {\n",
    "        'depth':               trial.suggest_int('depth', 4, 10),\n",
    "        'learning_rate':       trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'l2_leaf_reg':         trial.suggest_float('l2_leaf_reg', 1.0, 10.0, log=True),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n",
    "        'random_strength':     trial.suggest_float('random_strength', 0.0, 1.0),\n",
    "        'iterations': 1000, 'random_seed': 42,\n",
    "        'verbose': 0, 'early_stopping_rounds': 50, 'eval_metric': 'AUC',\n",
    "        'task_type': 'GPU' if DEVICE == 'gpu' else 'CPU',\n",
    "    }\n",
    "    model = CatBoostClassifier(**params)\n",
    "    model.fit(X_tr, y_tr, eval_set=(X_val, y_val))\n",
    "    return roc_auc_score(y_val, model.predict_proba(X_val)[:, 1])\n",
    "\n",
    "cat_study = optuna.create_study(direction='maximize')\n",
    "cat_study.optimize(cat_objective, n_trials=50, show_progress_bar=True)\n",
    "print(f'CAT  Best AUC: {cat_study.best_value:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_params = {**lgbm_study.best_params,\n",
    "               'n_estimators': 1000, 'random_state': 42, 'verbose': -1,\n",
    "               'device': DEVICE}\n",
    "\n",
    "lgbm_oof  = np.zeros(len(X))\n",
    "lgbm_test = np.zeros(len(X_test))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    model = lgb.LGBMClassifier(**lgbm_params)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)],\n",
    "              callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "    lgbm_oof[val_idx] = model.predict_proba(X_val)[:, 1]\n",
    "    lgbm_test += model.predict_proba(X_test)[:, 1] / cv.n_splits\n",
    "    print(f'Fold {fold+1}: AUC = {roc_auc_score(y_val, lgbm_oof[val_idx]):.4f}')\n",
    "\n",
    "print(f'LightGBM OOF AUC: {roc_auc_score(y, lgbm_oof):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {**xgb_study.best_params,\n",
    "              'n_estimators': 1000, 'random_state': 42, 'verbosity': 0,\n",
    "              'tree_method': 'hist', 'device': 'cuda' if DEVICE == 'gpu' else 'cpu',\n",
    "              'early_stopping_rounds': 50, 'eval_metric': 'auc'}\n",
    "\n",
    "xgb_oof  = np.zeros(len(X))\n",
    "xgb_test = np.zeros(len(X_test))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    model = xgb.XGBClassifier(**xgb_params)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    xgb_oof[val_idx] = model.predict_proba(X_val)[:, 1]\n",
    "    xgb_test += model.predict_proba(X_test)[:, 1] / cv.n_splits\n",
    "    print(f'Fold {fold+1}: AUC = {roc_auc_score(y_val, xgb_oof[val_idx]):.4f}')\n",
    "\n",
    "print(f'XGBoost  OOF AUC: {roc_auc_score(y, xgb_oof):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_params = {**cat_study.best_params,\n",
    "              'iterations': 1000, 'random_seed': 42,\n",
    "              'verbose': 0, 'early_stopping_rounds': 50, 'eval_metric': 'AUC',\n",
    "              'task_type': 'GPU' if DEVICE == 'gpu' else 'CPU'}\n",
    "\n",
    "cat_oof  = np.zeros(len(X))\n",
    "cat_test = np.zeros(len(X_test))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    model = CatBoostClassifier(**cat_params)\n",
    "    model.fit(X_train, y_train, eval_set=(X_val, y_val))\n",
    "    cat_oof[val_idx] = model.predict_proba(X_val)[:, 1]\n",
    "    cat_test += model.predict_proba(X_test)[:, 1] / cv.n_splits\n",
    "    print(f'Fold {fold+1}: AUC = {roc_auc_score(y_val, cat_oof[val_idx]):.4f}')\n",
    "\n",
    "print(f'CatBoost OOF AUC: {roc_auc_score(y, cat_oof):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_avg(*arrays):\n",
    "    ranked = [rankdata(a) / len(a) for a in arrays]\n",
    "    return np.mean(ranked, axis=0)\n",
    "\n",
    "ensemble_oof  = rank_avg(lgbm_oof, xgb_oof, cat_oof)\n",
    "ensemble_test = rank_avg(lgbm_test, xgb_test, cat_test)\n",
    "\n",
    "print(f'Ensemble OOF AUC: {roc_auc_score(y, ensemble_oof):.4f}')\n",
    "print(f'  LGBM OOF AUC:   {roc_auc_score(y, lgbm_oof):.4f}')\n",
    "print(f'  XGB  OOF AUC:   {roc_auc_score(y, xgb_oof):.4f}')\n",
    "print(f'  CAT  OOF AUC:   {roc_auc_score(y, cat_oof):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'Id': test_ids, 'Has_Valentine': ensemble_test})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(f'Saved. Mean prediction: {ensemble_test.mean():.4f}')\n",
    "\n",
    "from google.colab import files\n",
    "files.download('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
